# 一.spark基本概念

# 二.spark下载安装

# 三.spark交互运行
在这一节和下一节都是讲的在单机上面运行spark,因为最开始了解spark运行方式，单机就够了。

在命令行中，找到解压的spark文件夹，其中bin就是存储常见执行命令的文件夹啦，
使用`bin/pyspark` 就能够打开交互命令啦。如下图。

![](https://github.com/XierHacker/SparkTutorial/blob/master/cache/1.Basic/1.jpg)

我们这里使用一个非常简单统计文本的例子来跑一下spark,解压的spark文件夹下面，有个
`README.md`的文件，我们就统计这个文件的行数和第一行的内容。如下图,


在这个例子中，变量`file`　是一个弹性分布式数据集(Resilient Distributed Dataset,RDD),当然现在
你现在可以不用知道RDD到底是什么。

# 四.spark脚本运行
除了交互运行之外，还有脚本运行的方式，脚本运行的方式比交互运行要多几个步骤而已。

# 五.spark核心概念
从上层来看,每个 Spark 应用都由一个驱动器程序(driver program)来发起集群上的各种
并行操作。驱动器程序包含应用的 main 函数,并且定义了集群上的分布式数据集,还对这
些分布式数据集应用了相关操作。在前面的例子里,实际的驱动器程序就是 Spark shell 本
身,你只需要输入想要运行的操作就可以了。

**驱动器程序通过一个 `SparkContext` 对象来访问Spark。** 这个对象代表对计算集群的一个连
接。**shell 启动时已经自动创建了一个叫作sc的变量,这个变量本质上就是一个SparkContext 对象。** 我们可
以通过例 2-3 中的方法尝试输出 sc 来查看它的类型。

查看变量 sc
```
>>> sc
<pyspark.context.SparkContext object at 0x1025b8f90>
```

一旦有了 SparkContext,你就可以用它来创建 RDD。在例 2-1 和例 2-2 中,我们调用了
sc.textFile() 来创建一个代表文件中各行文本的 RDD。我们可以在这些行上进行各种操
作,比如 count() 。
要执行这些操作,驱动器程序一般要管理多个执行器(executor)节点。比如,如果我们在
集群上运行 count() 操作,那么不同的节点会统计文件的不同部分的行数。由于我们刚才是
在本地模式下运行 Spark shell,因此所有的工作会在单个节点上执行,但你可以将这个 shell
连接到集群上来进行并行的数据分析。

下图展示了 Spark 如何在一个集群上运行。

