# 一.spark基本概念
spark有什么用就不多啰嗦了,这里直接用机构来讲起吧,spark的结构
主要由下图展示的这样.

![]()

#### Spark Core
Spark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统
交互等模块。 Spark Core 中还包含了对弹性分布式数据集（resilient distributed dataset，简
称 RDD） 的 API 定义。 **RDD 表示分布在多个计算节点上可以并行操作的元素集合，是
Spark 主要的编程抽象。** Spark Core 提供了创建和操作这些集合的多个 API。

#### Spark SQL
Spark SQL 是Spark用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL
或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。 Spark SQL 支持多种数据源，比
如 Hive 表、 Parquet 以及 JSON 等。除了为 Spark 提供了一个 SQL 接口， Spark SQL 还支
持开发者将 SQL 和传统的 RDD 编程的数据操作方式相结合，不论是使用 Python、 Java 还
是 Scala， 开发者都可以在单个的应用中同时使用 SQL 和复杂的数据分析。通过与 Spark
所提供的丰富的计算环境进行如此紧密的结合， Spark SQL 得以从其他开源数据仓库工具
中脱颖而出。 Spark SQL 是在 Spark 1.0 中被引入的。
在 Spark SQL 之前，加州大学伯克利分校曾经尝试修改 Apache Hive 以使其运行在 Spark
上，当时的项目叫作 Shark。 现在，由于 Spark SQL 与 Spark 引擎和 API 的结合更紧密，
Shark 已经被 Spark SQL 所取代。

#### Spark Streaming
Spark Streaming 是 Spark 提供的对实时数据进行流式计算的组件。比如生产环境中的网页
服务器日志， 或是网络服务中用户提交的状态更新组成的消息队列，都是数据流。 Spark
Streaming 提供了用来操作数据流的 API， 并且与 Spark Core 中的 RDD API 高度对应。这
样一来，程序员编写应用时的学习门槛就得以降低，不论是操作内存或硬盘中的数据，还
是操作实时数据流， 程序员都更能应对自如。从底层设计来看， Spark Streaming 支持与
Spark Core 同级别的容错性、吞吐量以及可伸缩性。

#### MLlib
Spark 中还包含一个提供常见的机器学习（ML）功能的程序库，叫作 MLlib。 MLlib 提供
了很多种机器学习算法， 包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据
导入等额外的支持功能。 MLlib 还提供了一些更底层的机器学习原语，包括一个通用的梯
度下降优化算法。所有这些方法都被设计为可以在集群上轻松伸缩的架构。

#### GraphX
GraphX 是用来操作图（比如社交网络的朋友关系图）的程序库，可以进行并行的图计算。
与 Spark Streaming 和 Spark SQL 类似， GraphX 也扩展了 Spark 的 RDD API，能用来创建
一个顶点和边都包含任意属性的有向图。 GraphX 还支持针对图的各种操作（比如进行图4 ｜ 第 1 章
分割的 subgraph 和操作所有顶点的 mapVertices），以及一些常用图算法（比如 PageRank
和三角计数）。

####集群管理器
就底层而言， Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计
算。为了实现这样的要求， 同时获得最大灵活性， Spark 支持在各种集群管理器（cluster
manager） 上运行， 包括 Hadoop YARN、 Apache Mesos，以及 Spark 自带的一个简易调度
器，叫作独立调度器。 如果要在没有预装任何集群管理器的机器上安装 Spark，那么 Spark
自带的独立调度器可以让你轻松入门； 而如果已经有了一个装有 Hadoop YARN 或 Mesos
的集群，通过 Spark 对这些集群管理器的支持，你的应用也同样能运行在这些集群上。


# 二.spark下载安装

# 三.spark交互运行
在这一节和下一节都是讲的在单机上面运行spark,因为最开始了解spark运行方式，单机就够了。

在命令行中，找到解压的spark文件夹，其中bin就是存储常见执行命令的文件夹啦，
使用`bin/pyspark` 就能够打开交互命令啦。如下图。

![](https://github.com/XierHacker/SparkTutorial/blob/master/cache/1.Basic/1.jpg)

我们这里使用一个非常简单统计文本的例子来跑一下spark,解压的spark文件夹下面，有个
`README.md`的文件，我们就统计这个文件的行数和第一行的内容。如下图,
![](https://github.com/XierHacker/SparkTutorial/blob/master/cache/1.Basic/2.jpg)

在这个例子中，变量`file`　是一个弹性分布式数据集(Resilient Distributed Dataset,RDD),当然现在
你现在可以不用知道RDD到底是什么。

# 四.spark脚本运行
除了交互运行之外，还有脚本运行的方式，脚本运行的方式比交互运行要多几个步骤而已。

# 五.spark核心概念
从上层来看,每个 Spark 应用都由一个驱动器程序(driver program)来发起集群上的各种
并行操作。驱动器程序包含应用的 main 函数,并且定义了集群上的分布式数据集,还对这
些分布式数据集应用了相关操作。在前面的例子里,实际的驱动器程序就是 Spark shell 本
身,你只需要输入想要运行的操作就可以了。

**驱动器程序通过一个 `SparkContext` 对象来访问Spark。** 这个对象代表对计算集群的一个连
接。**shell 启动时已经自动创建了一个叫作sc的变量,这个变量本质上就是一个SparkContext 对象。** 我们可
以通过例 2-3 中的方法尝试输出 sc 来查看它的类型。

查看变量 sc
```
>>> sc
<pyspark.context.SparkContext object at 0x1025b8f90>
```

一旦有了 SparkContext,你就可以用它来创建 RDD。在例 2-1 和例 2-2 中,我们调用了
sc.textFile() 来创建一个代表文件中各行文本的 RDD。我们可以在这些行上进行各种操
作,比如 count() 。
要执行这些操作,驱动器程序一般要管理多个执行器(executor)节点。比如,如果我们在
集群上运行 count() 操作,那么不同的节点会统计文件的不同部分的行数。由于我们刚才是
在本地模式下运行 Spark shell,因此所有的工作会在单个节点上执行,但你可以将这个 shell
连接到集群上来进行并行的数据分析。

下图展示了 Spark 如何在一个集群上运行。

