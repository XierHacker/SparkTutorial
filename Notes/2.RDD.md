# 一.RDD概念
Spark 中的 RDD 就是**一个不可变的分布式对象集合。** 每个 RDD 都被分为多个分区,这些
分区运行在集群中的不同节点上。

**RDD 可以包含 Python、Java、Scala 中任意类型的对象,甚至可以包含用户自定义的对象。**

对数据的所有操作不外乎创建 RDD、转化已有 RDD 以及调用 RDD 操作进行求值,所以后面的内容就主要是
关于怎么创建RDD和怎么在RDD上进行操作。


# 二.创建RDD
Spark 提供了两种创建 RDD 的方式:读取外部数据集,以及在驱动器程序中对一个集合进
行并行化。
## Ⅰ.读取外部数据集
读取外部数据集的例子在上一个博客中就已经讲到了。就是从外部文件读取然后得到一个RDD.
比如上节的`file=sc.textFile(../README.md)`　就是从外部数据集创建RDD的例子。

## Ⅱ.并行化
并行化的这种方式，其实就是把程序中的一个已有的集合传给 SparkContext 的 parallelize()
方法,它让你可以在 shell 中快速创建出自己的 RDD,然后对这些 RDD 进行操作。比如下面这段代码。
```
lines = sc.parallelize(["pandas", "i like pandas"])
```
不过,需要注意的是,除了开发原型和测试时,这种方式用得并不多,毕竟这种方式需要把你的整个数据集先放在一台机器
的内存中。

**总结来说，使用的最多的还是从外部数据创建RDD的方法。**

# 三.RDD操作
首先说一个常用的几个操作,因为很多例子都会用到.怕到时候弄不明白,所以先列在这里.

**collect()**
>作用是把RDD中的所有元素作为一个列表返回.(所以,要是RDD中元素暴多的话,就别用这个了.)

>把数据返回驱动器程序中最简单、 最常见的操作是 collect()，它会将整个 RDD 的内容返
回。 collect() 通常在单元测试中使用，因为此时 RDD 的整个内容不会很大，可以放在内
存中。使用 collect() 使得 RDD 的值与预期结果之间的对比变得很容易。由于需要将数据
复制到驱动器进程中， collect() 要求所有数据都必须能一同放入单台机器的内存中。

**count()**
>作用:返回RDD中的元素个数

**countByValue()**
>作用:返回各元素在 RDD 中出现的次数


转化操作和行动操作的区别在于 Spark 计算 RDD 的方式不同。虽然你可以在任何时候定
义新的 RDD， 但 Spark 只会惰性计算这些 RDD。 **它们只有第一次在一个行动操作中用到
时，才会真正计算。**

**转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。**
 
## Ⅰ.转化操作
RDD 的转化操作是返回新 RDD 的操作。转化出来的 RDD 是惰性
求值的，**只有在行动操作中用到这些 RDD 时才会被计算。** 许多转化操作都是针对各个元
素的，也就是说，这些转化操作每次只会操作 RDD 中的一个元素。不过并不是所有的转
化操作都是这样的
常见的转化操作汇总:

### (1)在原RDD每个元素上面做操作返回新RDD
**map(f, preservesPartitioning=False)**

>作用:其中参数f表示传入的函数,map的作用就是在当前这个RDD的每个元素都作用一个
函数,然后把新的RDD返回.



>

>

## 2.行动操作
## 3.惰性求值
## 4.向spark传递函数
## .持久化


# 四.常见任务实战