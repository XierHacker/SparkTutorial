# 一.RDD概念
Spark 中的 RDD 就是**一个不可变的分布式对象集合。** 每个 RDD 都被分为多个分区,这些
分区运行在集群中的不同节点上。

**RDD 可以包含 Python、Java、Scala 中任意类型的对象,甚至可以包含用户自定义的对象。**

对数据的所有操作不外乎创建 RDD、转化已有 RDD 以及调用 RDD 操作进行求值,所以后面的内容就主要是
关于怎么创建RDD和怎么在RDD上进行操作。


# 二.创建RDD
Spark 提供了两种创建 RDD 的方式:读取外部数据集,以及在驱动器程序中对一个集合进
行并行化。
## 1.读取外部数据集
读取外部数据集的例子在上一个博客中就已经讲到了。就是从外部文件读取然后得到一个RDD.
比如上节的`file=sc.textFile(../README.md)`　就是从外部数据集创建RDD的例子。

## 2.并行化
并行化的这种方式，其实就是把程序中的一个已有的集合传给 SparkContext 的 parallelize()
方法,它让你可以在 shell 中快速创建出自己的 RDD,然后对这些 RDD 进行操作。比如下面这段代码。
```
lines = sc.parallelize(["pandas", "i like pandas"])
```
不过,需要注意的是,除了开发原型和测试时,这种方式用得并不多,毕竟这种方式需要把你的整个数据集先放在一台机器
的内存中。

**总结来说，使用的最多的还是从外部数据创建RDD的方法。**

# 三.RDD操作
## 1.转化操作
## 2.行动操作
## 3.惰性求值
## 4.向spark传递函数
## .持久化